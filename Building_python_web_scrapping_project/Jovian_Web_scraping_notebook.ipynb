{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Noteook for scraping data"
      ],
      "metadata": {
        "id": "LyLS-oXCY0cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jovian --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq6IC3UsYyUG",
        "outputId": "1ffc0f84-e8b3-4bf6-d1e5-36304e2fce2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jovian"
      ],
      "metadata": {
        "id": "C1Mk2KgiY6nL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jovian.commit(project=\"python-web-scraping-project-guide\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OixERSByZBwA",
        "outputId": "a1fd5bc2-35a1-40a9-b4ed-f1b95104fcaf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] jovian.commit() is no longer required on Google Colab. If you ran this notebook from Jovian, \n",
            "then just save this file in Colab using Ctrl+S/Cmd+S and it will be updated on Jovian. \n",
            "Also, you can also delete this cell, it's no longer necessary.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Top Repositories for Topics on GitHub\n",
        "\n",
        "TODO  (Intro):\n",
        "- Introduction about web scraping\n",
        "- Introduction about GitHub and the problem statement\n",
        "- Mention the tools you're using (Python, requests, Beautiful Soup, Pandas)\n"
      ],
      "metadata": {
        "id": "ylACS0pJZScj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps we'll follow:\n",
        "\n",
        "- We're going to scrape https://github.com/topics\n",
        "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "- For each topic we'll create a CSV file in the following format:\n",
        "\n",
        "```\n",
        "Repo Name,Username,Stars,Repo URL\n",
        "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
        "```"
      ],
      "metadata": {
        "id": "2CFVOvsLZVTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape the list of topics from Github\n",
        "\n",
        "Explain how you'll do it.\n",
        "\n",
        "- use requests to downlaod the page\n",
        "- user BS4 to parse and extract information\n",
        "- convert to a Pandas dataframe\n",
        "\n",
        "Let's write a function to download the page."
      ],
      "metadata": {
        "id": "4Q4JHlSkZdy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_topics_page():\n",
        "    # TODO - add comments\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return doc"
      ],
      "metadata": {
        "id": "18gPIo21Zbr1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add some explanation"
      ],
      "metadata": {
        "id": "iBABm9WsZkiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = get_topics_page()"
      ],
      "metadata": {
        "id": "wLroniB8ZmIb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some helper functions to parse information from the page.\n",
        "\n",
        "To get topic titles, we can pick `p` tags with the `class` ...\n",
        "\n",
        "![](https://i.imgur.com/OnzIdyP.png)"
      ],
      "metadata": {
        "id": "EK4M6pMGZw-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_titles(doc):\n",
        "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
        "    topic_titles = []\n",
        "    for tag in topic_title_tags:\n",
        "        topic_titles.append(tag.text)\n",
        "    return topic_titles"
      ],
      "metadata": {
        "id": "OFdYEN0OZygg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_topic_titles` can be used to get the list of titles"
      ],
      "metadata": {
        "id": "IzDfAgK5Z1TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles = get_topic_titles(doc)"
      ],
      "metadata": {
        "id": "YGbw1lbSZ3za"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR2OhTXOZ6h2",
        "outputId": "459f5d5d-fc01-41c7-b0fd-003c5899b88b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WEAYW81Z9O4",
        "outputId": "f587900b-ddbe-43f6-9583-45070f07a192"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Awesome Lists', 'Chrome', 'Code quality', 'Compiler', 'CSS']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly we have defined functions for descriptions and URLs."
      ],
      "metadata": {
        "id": "ZaHUmukQaDX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_descs(doc):\n",
        "    desc_selector = 'f5 color-text-secondary mb-0 mt-1'\n",
        "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
        "    topic_descs = []\n",
        "    for tag in topic_desc_tags:\n",
        "        topic_descs.append(tag.text.strip())\n",
        "    return topic_descs\n"
      ],
      "metadata": {
        "id": "vY4th847aEar"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - example and explanation"
      ],
      "metadata": {
        "id": "RWDIpkIoaIV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_urls(doc):\n",
        "    topic_link_tags = doc.find_all('a', {'class': 'd-flex no-underline'})\n",
        "    topic_urls = []\n",
        "    base_url = 'https://github.com'\n",
        "    for tag in topic_link_tags:\n",
        "        topic_urls.append(base_url + tag['href'])\n",
        "    return topic_urls"
      ],
      "metadata": {
        "id": "J5B0ARg6aOaL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put this all together into a single function"
      ],
      "metadata": {
        "id": "fHadj8T1aptp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics():\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    topics_dict = {\n",
        "        'title': get_topic_titles(doc),\n",
        "        'description': get_topic_descs(doc),\n",
        "        'url': get_topic_urls(doc)\n",
        "    }\n",
        "    return pd.DataFrame(topics_dict)"
      ],
      "metadata": {
        "id": "nucgn1WEan2k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jovian"
      ],
      "metadata": {
        "id": "WLa7CHbUaxN-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jovian.commit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuWjDLixaz-o",
        "outputId": "ffa85883-6190-4f35-a4fe-1e3ec853b945"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] jovian.commit() is no longer required on Google Colab. If you ran this notebook from Jovian, \n",
            "then just save this file in Colab using Ctrl+S/Cmd+S and it will be updated on Jovian. \n",
            "Also, you can also delete this cell, it's no longer necessary.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the top 25 repositories from a topic page\n",
        "\n",
        "TODO - explanation and step"
      ],
      "metadata": {
        "id": "wa380grPa4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_page(topic_url):\n",
        "    # Download the page\n",
        "    response = requests.get(topic_url)\n",
        "    # Check successful response\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    # Parse using Beautiful soup\n",
        "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return topic_doc"
      ],
      "metadata": {
        "id": "QkKjkHjxa1AV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = get_topic_page('https://github.com/topics/3d')"
      ],
      "metadata": {
        "id": "Bfp8b-_ca8o4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - talk about the h1 tags"
      ],
      "metadata": {
        "id": "gqEvIklFbBhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_repo_info(h1_tag, star_tag):\n",
        "    # returns all the required info about a repository\n",
        "    a_tags = h1_tag.find_all('a')\n",
        "    username = a_tags[0].text.strip()\n",
        "    repo_name = a_tags[1].text.strip()\n",
        "    repo_url =  base_url + a_tags[1]['href']\n",
        "    stars = parse_star_count(star_tag.text.strip())\n",
        "    return username, repo_name, stars, repo_url"
      ],
      "metadata": {
        "id": "vOU3VNTQa_NY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - show a example"
      ],
      "metadata": {
        "id": "tt_okux3bFUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_repos(topic_doc):\n",
        "    # Get the h1 tags containing repo title, repo URL and username\n",
        "    h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
        "    repo_tags = topic_doc.find_all('h1', {'class': h1_selection_class} )\n",
        "    # Get star tags\n",
        "    star_tags = topic_doc.find_all('a', { 'class': 'social-count float-none'})\n",
        "\n",
        "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
        "\n",
        "    # Get repo info\n",
        "    for i in range(len(repo_tags)):\n",
        "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
        "        topic_repos_dict['username'].append(repo_info[0])\n",
        "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
        "        topic_repos_dict['stars'].append(repo_info[2])\n",
        "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "    return pd.DataFrame(topic_repos_dict)"
      ],
      "metadata": {
        "id": "2t4y8nB0bd73"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - show an example"
      ],
      "metadata": {
        "id": "xijoZlyLbf7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic(topic_url, path):\n",
        "    if os.path.exists(path):\n",
        "        print(\"The file {} already exists. Skipping...\".format(path))\n",
        "        return\n",
        "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "    topic_df.to_csv(path, index=None)"
      ],
      "metadata": {
        "id": "c5fkD6sLbkqs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "- We have a funciton to get the list of topics\n",
        "- We have a function to create a CSV file for scraped repos from a topics page\n",
        "- Let's create a function to put them together"
      ],
      "metadata": {
        "id": "f_u7Fod_brM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics_repos():\n",
        "    print('Scraping list of topics')\n",
        "    topics_df = scrape_topics()\n",
        "\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    for index, row in topics_df.iterrows():\n",
        "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
        "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
      ],
      "metadata": {
        "id": "j2c2oYwvbsuo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
      ],
      "metadata": {
        "id": "Cpan5VdCcOdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "scrape_topics_repos()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "D49764RwcROF",
        "outputId": "a8b2a02c-d2d5-44e7-faa5-be1d7997641e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping list of topics\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-410248858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscrape_topics_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-383087020.py\u001b[0m in \u001b[0;36mscrape_topics_repos\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_topics_repos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping list of topics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtopics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3506935891.py\u001b[0m in \u001b[0;36mscrape_topics\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_topic_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like you’re constructing a pandas DataFrame (or similar structure) from multiple arrays/lists, and one or more of those arrays have a different length than the others. Pandas requires all columns (or 1-D arrays that become columns) to have the same number of elements when you build the DataFrame from a dict or a list of arrays.\n",
        "\n",
        "What the traceback tells you\n",
        "- The error originates in pandas internals when it tries to align multiple 1-D inputs into a 2-D structure.\n",
        "- Specifically, the message: \"All arrays must be of the same length\" means at least one of the arrays you’re passing has a length that differs from the others.\n",
        "\n",
        "Common causes\n",
        "- Mismatched lengths in the data sources you’re aggregating (e.g., topics, repos, descriptions, URLs).\n",
        "- A filtering operation reduced one list but not the others.\n",
        "- A miscount when you’re iterating and appending to lists inside a loop (off-by-one or conditional path skipping items).\n",
        "- Returning different numbers of items from a function for each field.\n",
        "\n",
        "How to diagnose and fix\n",
        "1. Inspect lengths before creating the DataFrame\n",
        "   - Print lengths of all arrays/lists you plan to combine:\n",
        "     - `print(len(topics), len(repos), len(descriptions), len(urls))`\n",
        "   - If you’re building from a DataFrame or a dict, check each value’s length:\n",
        "     - `for key, value in data.items(): print(key, len(value) if hasattr(value, '__len__') else 'N/A')`\n",
        "\n",
        "2. Find where the mismatch occurs\n",
        "   - If you’re inside a function like `scrape_topics_repos()`, add debug statements or use a small subset:\n",
        "     - Collect items in a list of dicts, then `pd.DataFrame(list_of_dicts)`; this often helps identify which field is short.\n",
        "   - Compare lengths after each processing step (e.g., after filtering, deduplication, or splitting strings).\n",
        "\n",
        "3. Ensure consistent construction\n",
        "   - Build a list of records (dicts) and convert once:\n",
        "     - ```\n",
        "       records = []\n",
        "       for item in items:\n",
        "           rec = {\n",
        "               \"topic\": item.topic,\n",
        "               \"repo\": item.repo,\n",
        "               \"description\": item.description,\n",
        "               \"url\": item.url\n",
        "           }\n",
        "           records.append(rec)\n",
        "       df = pd.DataFrame(records)\n",
        "       ```\n",
        "   - If you must use separate lists, ensure they’re appended in lockstep:\n",
        "     - ```\n",
        "       topics, repos, descriptions, urls = [], [], [], []\n",
        "       for item in items:\n",
        "           topics.append(item.topic)\n",
        "           repos.append(item.repo)\n",
        "           descriptions.append(item.description)\n",
        "           urls.append(item.url)\n",
        "       # then:\n",
        "       df = pd.DataFrame({\"topic\": topics, \"repo\": repos, \"description\": descriptions, \"url\": urls})\n",
        "       ```\n",
        "\n",
        "4. Handle missing data gracefully\n",
        "   - If some fields are missing for certain items, decide on a strategy:\n",
        "     - Pad with None/NaN to maintain equal lengths.\n",
        "     - Use an inner join approach when merging multiple sources so only complete records are kept.\n",
        "\n",
        "5. Quick checks you can run\n",
        "   - After collecting each field, print the lengths:\n",
        "     - `print(len(topics), len(repos), len(descriptions), len(urls))`\n",
        "   - If you’re parsing HTML, ensure your selectors consistently return results for every item; missing selectors can cause empty lists for some fields.\n",
        "\n",
        "If you share a snippet of the relevant code (how you build the lists and how you construct the DataFrame), I can pinpoint the exact mismatch and suggest a precise fix."
      ],
      "metadata": {
        "id": "ltfxOm3Dc9M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check that the CSVs were created properly"
      ],
      "metadata": {
        "id": "msVwLbJDcYcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read and display a CSV using Pandas"
      ],
      "metadata": {
        "id": "7Xs2RyVRcaUt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Future Work\n",
        "\n",
        "Summary of what we did\n",
        "\n",
        "- ?\n",
        "- ?\n",
        "\n",
        "\n",
        "References to links you found useful\n",
        "\n",
        "- ?\n",
        "- ?\n",
        "\n",
        "Ideas for future work\n",
        "\n",
        "- ?\n",
        "- ?"
      ],
      "metadata": {
        "id": "0Vq5A2zhco3d"
      }
    }
  ]
}